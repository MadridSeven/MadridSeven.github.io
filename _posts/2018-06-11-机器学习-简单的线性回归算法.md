---
layout: post
title: "机器学习--简单的线性回归算法"
date: 2018-06-11
description: "人工智能，机器学习，深度学习,机器学习"
tag: 人工智能

---

### 写在前面
&emsp;&emsp;前一段时间一直忙于毕业答辩，完了还去桂林完了一圈，算是毕业旅行了吧。今天终于有空来更新了，
之前的那篇博客主要说了关于机器学习的一些概念性的知识，阐述了监督学习和无监督学习的概念，这篇博客我会介绍这个系列
的第一个算法内容：线性回归算法，并且阐述代价函数的定义。如果要流畅的阅读本文，还是需要一点高数的基础知识的，不过也不难。让我们开始吧！

### 线性回归
&emsp;&emsp;之前的那片博客为了说明监督学习的概念我们举了一个关于房价的例子，如下图：

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fs75schp27j30et07vdh4.jpg)

&emsp;&emsp;图中横坐标代表房子的价格，纵坐标代表房子的面积，其中的红色叉号表示我们训练集中的数据，我们的目标是在这些叉号的基础上拟合一条直线。
这是一个监督学习的例子，因为我们给出了一系列的正确答案(叉号)用来做训练集，更具体来说这是一个回归问题，回归一词指的是，我们根据之前的数据预测出一个准确的输出值，
同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，比如说上篇博客中所说的预测肿瘤为良性还是恶性，这就是一个0/1离散输出问题。

&emsp;&emsp;假设就预测房价问题来说，我们有像下图这样的一个训练集

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fs7afqdo97j308s03ugli.jpg)

&emsp;&emsp;另外为了之后方便阅读，这里定义一些参数：

- m 代表训练集中实例的数量
- x 代表特征/输入变量
- y 代表目标变量/输出变量
- (x,y) 代表训练集中的实例
- (x(i),y(i) ) 代表第 i 个观察实例
- h 代表学习算法的解决方案或函数也称为假设（hypothesis）

&emsp;&emsp;在监督学习中，学习算法工作的整个过程如下图所示。我们先将训练集里的数据喂给我们的学习算法，学习算法会输出一个函数，通常表示为小写h
。这个函数的输入是输入是房屋尺寸大小，输出一个 y 值对应房子的价格，因此，h 是一个从
x 到y 的函数映射。

![](http://ww1.sinaimg.cn/mw690/006CsMmSgy1fs7awhdg4rj30ac06fgli.jpg)

&emsp;&emsp;因而，要解决房价预测问题，我们实际上
是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋
的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，
对于我们的房价预测问题，我们该如何表达h？

&emsp;&emsp;在这里我们直接引出单变量线性回归方程 <a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" title="h$_\Theta $=\Theta $_0$+\Theta $_1$x" /></a> (其实就是简单的一元一次方程)这样的问题叫作单变量线性回归问题。

&emsp;&emsp;方程中 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a> 代表两个参数，其实分别就是就是函数的直线的斜率和直线同y轴的切点，只要确定了这两个参数这个函数也就随之确定了，那么我们该如何确定这两个模型参数呢？

### 代价函数

&emsp;&emsp; 我们的目标是使得 <a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta" title="h$_\Theta $" /></a> 输出的值尽量的与实际值 y 的误差最小，而这个误差可以用一个函数来表示，这里我们把它称为代价函数。下面这个式子就是代价函数的公式，大家先看一眼。

<a href="http://www.codecogs.com/eqnedit.php?latex=J&space;(&space;\Theta&space;$_0$,\Theta&space;$_1$&space;)=1/2m\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?J&space;(&space;\Theta&space;$_0$,\Theta&space;$_1$&space;)=1/2m\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" title="J ( \Theta $_0$,\Theta $_1$ )=1/2m\sum_{i=1}^{m}(h$_\Theta $(x$^{(i)}$)-y$^{(i)}$)^{2}" /></a>        &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;①



<a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" title="h$_\Theta $=\Theta $_0$+\Theta $_1$x" /></a>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;②

&emsp;&emsp;上面的两个公式中第一个式子就是我们的代价函数，第二个式子是单变量线性回归的公式，在这里解释下第一个式子的细节：在 ① 式中 <a href="http://www.codecogs.com/eqnedit.php?latex=(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" title="(h$_\Theta $(x$^{(i)}$)-y$^{(i)}$)^{2}" /></a> 为 <a href="http://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$(x$^{(i)}$)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?h$_\Theta&space;$(x$^{(i)}$)" title="h$_\Theta $(x$^{(i)}$)" /></a> 和 <a href="http://www.codecogs.com/eqnedit.php?latex=y$^{(i)}$" target="_blank"><img src="http://latex.codecogs.com/gif.latex?y$^{(i)}$" title="y$^{(i)}$" /></a> 的平方误差，<a href="http://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)})^{2}&space;$" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)})^{2}&space;$" title="\sum_{i=1}^{m}(h$_\Theta $(x$^{(i)}$)-y$^{(i)})^{2} $" /></a> 为平方误差的总和，我们要做的就是尽可能的让这个平方误差降低到最小，通过调整<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a>的值。而右式最前面的1/2m则是为了方便积分加上去的，关于这个之后说到的时候你们就会明白了。<font color="red">而这个式子就是平方误差代价函数，这是解决回归问题的最常用手段。</font>




----------
<font color="blue">笔者水平有限，若有错漏，欢迎指正，如果转载以及CV操作，请务必注明出处，谢谢！</font>


----------


<font color="red">版权声明：本文为博主原创文章，未经博主允许不得转载。</font>
