---
layout: post
title: "机器学习--简单的线性回归算法"
date: 2018-06-11
description: "人工智能，机器学习，深度学习,机器学习"
tag: 人工智能

---

### 写在前面
&emsp;&emsp;前一段时间一直忙于毕业答辩，完了还去桂林完了一圈，算是毕业旅行了吧。今天终于有空来更新了，
之前的那篇博客主要说了关于机器学习的一些概念性的知识，阐述了监督学习和无监督学习的概念，这篇博客我会介绍这个系列
的第一个算法内容：线性回归算法，并且阐述代价函数的定义。如果要流畅的阅读本文，还是需要一点高数的基础知识的，不过也不难。让我们开始吧！

### 线性回归
&emsp;&emsp;之前的那片博客为了说明监督学习的概念我们举了一个关于房价的例子，如下图：

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fs75schp27j30et07vdh4.jpg)

&emsp;&emsp;图中横坐标代表房子的价格，纵坐标代表房子的面积，其中的红色叉号表示我们训练集中的数据，我们的目标是在这些叉号的基础上拟合一条直线。
这是一个监督学习的例子，因为我们给出了一系列的正确答案(叉号)用来做训练集，更具体来说这是一个回归问题，回归一词指的是，我们根据之前的数据预测出一个准确的输出值，
同时，还有另一种最常见的监督学习方式，叫做分类问题，当我们想要预测离散的输出值，比如说上篇博客中所说的预测肿瘤为良性还是恶性，这就是一个0/1离散输出问题。

&emsp;&emsp;假设就预测房价问题来说，我们有像下图这样的一个训练集

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fs7afqdo97j308s03ugli.jpg)

&emsp;&emsp;另外为了之后方便阅读，这里定义一些参数：

- m 代表训练集中实例的数量
- x 代表特征/输入变量
- y 代表目标变量/输出变量
- (x,y) 代表训练集中的实例
- (x(i),y(i) ) 代表第 i 个观察实例
- h 代表学习算法的解决方案或函数也称为假设（hypothesis）

&emsp;&emsp;在监督学习中，学习算法工作的整个过程如下图所示。我们先将训练集里的数据喂给我们的学习算法，学习算法会输出一个函数，通常表示为小写h
。这个函数的输入是输入是房屋尺寸大小，输出一个 y 值对应房子的价格，因此，h 是一个从
x 到y 的函数映射。

![](http://ww1.sinaimg.cn/mw690/006CsMmSgy1fs7awhdg4rj30ac06fgli.jpg)

&emsp;&emsp;因而，要解决房价预测问题，我们实际上
是要将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋
的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。那么，
对于我们的房价预测问题，我们该如何表达h？

&emsp;&emsp;在这里我们直接引出单变量线性回归方程 <a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" title="h$_\Theta $=\Theta $_0$+\Theta $_1$x" /></a> (其实就是简单的一元一次方程)这样的问题叫作单变量线性回归问题。

&emsp;&emsp;方程中 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a> 代表两个参数，其实分别就是就是函数的直线的斜率和直线同y轴的切点，只要确定了这两个参数这个函数也就随之确定了，那么我们该如何确定这两个模型参数呢？

### 代价函数

&emsp;&emsp; 我们的目标是使得 <a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$=\Theta&space;$_0$&plus;\Theta&space;$_1$x" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta" title="h$_\Theta $" /></a> 输出的值尽量的与实际值 y 的误差最小，而这个误差可以用一个函数来表示，这里我们把它称为代价函数。

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fsar1g6wdbj30hx09m3yz.jpg)

&emsp;&emsp;上图中第三行的式子就是我们的代价函数，第一行的式子是单变量线性回归的公式，最后一行是我们的目标，就是通过更改<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a>的值，使得代价函数取到最小值。在这里解释下代价函数公式的细节：在式中 <a href="http://www.codecogs.com/eqnedit.php?latex=(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)}$)^{2}" title="(h$_\Theta $(x$^{(i)}$)-y$^{(i)}$)^{2}" /></a> 为 <a href="http://www.codecogs.com/eqnedit.php?latex=h$_\Theta&space;$(x$^{(i)}$)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?h$_\Theta&space;$(x$^{(i)}$)" title="h$_\Theta $(x$^{(i)}$)" /></a> 和 <a href="http://www.codecogs.com/eqnedit.php?latex=y$^{(i)}$" target="_blank"><img src="http://latex.codecogs.com/gif.latex?y$^{(i)}$" title="y$^{(i)}$" /></a> 的平方误差，<a href="http://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)})^{2}&space;$" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\sum_{i=1}^{m}(h$_\Theta&space;$(x$^{(i)}$)-y$^{(i)})^{2}&space;$" title="\sum_{i=1}^{m}(h$_\Theta $(x$^{(i)}$)-y$^{(i)})^{2} $" /></a> 为平方误差的总和，我们要做的就是尽可能的让这个平方误差降低到最小，通过调整<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a>的值。而右式最前面的1/2m则是为了方便积分加上去的，关于这个之后说到的时候你们就会明白了。<font color="red">而这个式子就是平方误差代价函数，这是解决回归问题的最常用手段。</font>


&emsp;&emsp;大多数朋友看到这里应该对代价函数到底能做什么还是一头雾水，这里为了说明代价函数的作用我们举个例子：

&emsp;&emsp;我们设 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;&_0&&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;&_0&&space;=&space;0" title="\Theta &_0& = 0" /></a> ，那么我们的公式就变成了下图右侧的样子：

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fsarai0nouj30ji0a2wf6.jpg)

&emsp;&emsp;我们设训练集为(1,1),(2,2),(3,3),分别画出<a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta$(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta$(x)" title="h$_\Theta$(x)" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;&_1&)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;&_1&)" title="J(\Theta &_1&)" /></a>对应的图像如下图，我们发现在<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a>取值为1的时候，<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;&_1&)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;&_1&)" title="J(\Theta &_1&)" /></a>取到最小值，此时<a href="https://www.codecogs.com/eqnedit.php?latex=h$_\Theta$(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h$_\Theta$(x)" title="h$_\Theta$(x)" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;&_1&)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;&_1&)" title="J(\Theta &_1&)" /></a>也正好符号训练集的数据。


![](http://ww1.sinaimg.cn/large/006CsMmSgy1fsarvbf02zj30jb0ast9u.jpg)

&emsp;&emsp;当考虑到2个参数值<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a>时，直观表达就需要3D视图了，如下图，所以当参数变的越来越多时，我们将用矩阵才表示这种关系。

![](http://ww1.sinaimg.cn/large/006CsMmSgy1fsas5j1knmj30h20a6aci.jpg)

&emsp;&emsp;我们从上图中可以看到在三维空间中存在一个使得<a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta&space;&_0&,\Theta&space;&_1&)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;&_0&,\Theta&space;&_1&)" title="J(\Theta &_0&,\Theta &_1&)" /></a>最小的点，通过这些图形，我希望大家能够体会到代价函数所表达的值到底是什么意思，当然我们也不希望每次都像这样画图去确定这些点的数值，因为之后的大多数情况下，我们遇到的是更复杂、更高维度、更多参数的情况，这些情况是很难画图出来的，我们需要的是通过一种算法，然后我们编写程序自动地找出能使代价函数J 最小化的参数 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_0$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_0$" title="\Theta $_0$" /></a> 和 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;$_1$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;$_1$" title="\Theta $_1$" /></a> 的值。下一篇博客中的梯度下降算法就是这样一个算法。



----------
<font color="blue">笔者水平有限，若有错漏，欢迎指正，如果转载以及CV操作，请务必注明出处，谢谢！</font>


----------


<font color="red">版权声明：本文为博主原创文章，未经博主允许不得转载。</font>
